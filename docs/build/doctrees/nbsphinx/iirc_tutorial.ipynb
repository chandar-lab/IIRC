{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIRC Package Tutorial Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from iirc.lll_dataset.torch_lll_dataset import TorchLLLDataset\n",
    "from iirc.iirc_definitions import IIRC_SETUP, CIL_SETUP, NO_LABEL_PLACEHOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CIL setup (Class Incremental Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mock dataset of 120 samples, with each belonging to one of the four classes **A** - **B** - **C** - **D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 120\n",
    "n_per_y = 30\n",
    "x = np.random.rand(n,32,32,3)\n",
    "y = [\"A\"] * n_per_y + [\"B\"] * n_per_y + [\"C\"] * n_per_y + [\"D\"] * n_per_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this dataset should be converted to the format used by the lll_datasets, where images should be a pillow images (or strings representing the images path in case of large datasets, such as ImageNet), and the dataset should be arranged as a list of tuples of the form (image, (label,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.uint8(x*255)\n",
    "mock_dataset = [(Image.fromarray(x[i], 'RGB'), (y[i],)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a tasks schedule, where the first task introduces the labels **A** and **B**, and the second task introduces **C** and **D** (tasks don't need to be of equal size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [[\"A\", \"B\"], [\"C\", \"D\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a transformations function that takes the image and converts it to a tensor, as well as normalize the image, apply augmentations, etc.\n",
    "\n",
    "There are two such functions that can be provided: *essential_transforms_fn* and *augmentation_transforms_fn*\n",
    "\n",
    "If *augmentation_transforms_fn* is provided, it will always be applied except if the *TorchLLLDataset* is told not to apply augmentations in a specific context (we will see later how)\n",
    "\n",
    "Otherwise, *essential_transforms_fn* will be applied\n",
    "\n",
    "So for example in a test set where augmentations are not needed, *augmentation_transforms_fn* shouldn't be provided in the *TorchLLLDataset* initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "essential_transforms_fn = transforms.ToTensor()\n",
    "augmentation_transforms_fn = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to initialize the incremental dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cil_dataset = TorchLLLDataset(dataset=mock_dataset, tasks=tasks, setup=CIL_SETUP, \n",
    "                              essential_transforms_fn=essential_transforms_fn, \n",
    "                              augmentation_transforms_fn=augmentation_transforms_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can make use of *cil_dataset* by choosing the task we wish to train on using the *choose_task(task_id)* function, and creating a dataloader out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cil_dataset.choose_task(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print the length of the dataset, we will only get the length of the samples that belong to the current task. The same goes for fetching a sample, as we only have access to the samples of the current task. This means that indexing the *cil_dataset* is relative to the current task, so the 0th sample for example will be different when we choose another task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(cil_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5137, 0.7647, 0.7412],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1725, 0.4000, 0.9922],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2549, 0.7137, 0.3373]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.7804, 0.6314, 0.9529],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.8471, 0.8471, 0.8196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.6000, 0.4235]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2157, 0.0941, 0.6118],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.4000, 0.0706, 0.3529],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.8549, 0.1765, 0.3451]]]),\n",
       " 'A',\n",
       " 'None')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cil_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen the format returned when we fetch a sample from the dataset is a tuple of length 3 consisting of (image tensor, image label, NO_LABEL_PLACEHOLDER), NO_LABEL_PLACEHOLDER is set to the string \"None\" and should be ignored in the class incremental setup.\n",
    "\n",
    "We can also access what classes have we seen thus far (they don't reset if a previous task is re-chosen, also they are unordered):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0, current task: ['A', 'B'], dataset length: 60, classes seen: ['B', 'A']\n",
      "Task 1, current task: ['C', 'D'], dataset length: 60, classes seen: ['D', 'B', 'C', 'A']\n"
     ]
    }
   ],
   "source": [
    "cil_dataset.reset()\n",
    "cil_dataset.choose_task(0) \n",
    "print(f\"Task {cil_dataset.cur_task_id}, current task: {cil_dataset.cur_task}, dataset length: {len(cil_dataset)}, classes seen: {cil_dataset.seen_classes}\")\n",
    "cil_dataset.choose_task(1)\n",
    "print(f\"Task {cil_dataset.cur_task_id}, current task: {cil_dataset.cur_task}, dataset length: {len(cil_dataset)}, classes seen: {cil_dataset.seen_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to load the data of all the tasks up to a specific task (including it), this can be done using the *load_tasks_up_to(task_id)* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current task: ['A', 'B', 'C', 'D'], dataset length: 120\n"
     ]
    }
   ],
   "source": [
    "cil_dataset.load_tasks_up_to(1)\n",
    "print(f\"current task: {cil_dataset.cur_task}, dataset length: {len(cil_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a sample needs to be accessed in it's original form (PIL form) without any transformations, for example to be added to the replay buffer, the *get_item(index)* method can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32 at 0x286B762A460>, 'A', 'None')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cil_dataset.get_item(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if we need to know the indices of the samples that belong to a specific class, this can be done using the *get_image_indices_by_cla(class name)* method, however, this can only be done if that class belongs to the current task. Moreover, these indices are relative to the current task, so whenever we change the task, they would point to totally different samples in the new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  7, 29, 26, 13,  2,  6, 15, 27, 21, 19, 10,  5,  9,  4, 14,  3,\n",
       "       22, 20,  0, 23, 28, 18, 11, 12,  8, 16, 24, 25,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cil_dataset.get_image_indices_by_cla(\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If *cil_dataset* uses data augmentations, but we needed to disable them in a specific part of the code, this context manager can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cil_dataset.disable_augmentations():\n",
    "    # any samples loaded here will have the essential transformations function applied to them\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to load this dataset in minibatches for training, the torch dataloader can be used with it, **but don't forget to reinstantiate the dataloader whenever the task changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.6235, 0.8745, 0.3255],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.2118, 0.9804, 0.7490],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.4235, 0.9373]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.1882, 0.4745, 0.4510],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.4353, 0.6039, 0.6863],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.2118, 0.5373, 0.1333]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.8471, 0.8196, 0.3373],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.1765, 0.8118, 0.7686],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.8510, 0.6980]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.4745, 0.9294,  ..., 0.0549, 0.8471, 0.0000],\n",
       "           [0.0000, 0.2118, 0.1882,  ..., 0.8863, 0.1765, 0.5647],\n",
       "           [0.0000, 0.6667, 0.4118,  ..., 0.5176, 0.7922, 0.4275]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.8314, 0.0353,  ..., 0.6000, 0.7176, 0.6549],\n",
       "           [0.0000, 0.8314, 0.2196,  ..., 0.2157, 0.9333, 0.6275],\n",
       "           [0.0000, 0.8549, 0.6667,  ..., 0.0431, 0.4039, 0.9451]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.2941, 0.6039,  ..., 0.4863, 0.2471, 0.7765],\n",
       "           [0.0000, 0.9725, 0.9137,  ..., 0.3804, 0.1451, 0.2275],\n",
       "           [0.0000, 0.6745, 0.1216,  ..., 0.1412, 0.8078, 0.8353]]]]),\n",
       " ('B', 'A'),\n",
       " ('None', 'None')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "cil_dataset.choose_task(0) \n",
    "train_loader = DataLoader(cil_dataset, batch_size=2, shuffle=True)\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using IIRC setup (Incremental Implicitly Refined Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Imaging that samples belonging to class **A** have also another sublabel of either **Aa** or **Ab**, and the samples belonging to class **B** have another sublabel of either **Ba** or **Bb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('A', 'Aa'), ('A', 'Ab'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('B', 'Ba'), ('B', 'Bb'), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('C',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',), ('D',)]\n"
     ]
    }
   ],
   "source": [
    "y_iirc = []\n",
    "for i, label in enumerate(y):\n",
    "    if label == \"A\" and (i % 2) == 0:\n",
    "        y_iirc.append((\"A\", \"Aa\"))\n",
    "    elif label == \"A\":\n",
    "        y_iirc.append((\"A\", \"Ab\"))\n",
    "    elif label == \"B\" and (i % 2) == 0:\n",
    "        y_iirc.append((\"B\", \"Ba\"))\n",
    "    elif label == \"B\":\n",
    "        y_iirc.append((\"B\", \"Bb\"))\n",
    "    else:\n",
    "        y_iirc.append((label,))\n",
    "print(y_iirc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_dataset_iirc = [(Image.fromarray(x[i], 'RGB'), y_iirc[i]) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32 at 0x286B76358E0>, ('A', 'Aa'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_dataset_iirc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's redefine the tasks by incorporating the new subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_iirc = [[\"A\", \"B\", \"C\"], [\"Aa\", \"Ba\", \"D\"], [\"Ab\", \"Bb\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functionality that we mentioned in the CIL setup is still applicable here, but there are some differences though.\n",
    "\n",
    "The first difference is that in the training set, we don't necessarily need all the samples that belong to both labels **A** and **Aa** to be seen across the two tasks, what makes more sense is that some of them need to appear in the first task and some others need to appear in the second task, probably with some overlap\n",
    "\n",
    "Hence we have the two arguments *superclass_data_pct* and *subclass_data_pct*:\n",
    "* *superclass_data_pct* controls the percentage of the samples that belong to **A** that will appear when **A** is introduced\n",
    "* *subclass_data_pct* controls the percentage of the samples that belong to **Aa** that will appear when **Aa** is introduced (same for other subclasses)\n",
    "\n",
    "So in this example we have 15 samples that have the labels (**A**, **Aa**), and 15 samples that have the labels (**A**, **Ab**):\n",
    "* *superclass_data_pct* = 1.0, *subclass_data_pct* = 1.0: \n",
    "This means that all 30 samples with label **A** will appear in the first task, then all 15 samples with label **Aa** will appear in the second task, etc (100% overlab)\n",
    "* *superclass_data_pct* = 0.5, *subclass_data_pct* = 0.5: \n",
    "15 samples with label **A** will appear in the first task, then 8 samples with label **Aa** will appear in the second task, etc (no overlab)\n",
    "* *superclass_data_pct* = 0.6, *subclass_data_pct* = 0.8: \n",
    "18 samples with label **A** will appear in the first task, then 12 samples with label **Aa** will appear in the second task, etc (40% overlab)\n",
    "\n",
    "This procedure will only be done if the argument *test_mode* is set to *False*, as in the test set we don't care about this kind of data repetition but we care more about evaluating on all the available samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of task 0 training data: 66, number of samples for class A: 18\n",
      "Length of task 0 training data: 66, number of samples for class A: 30\n"
     ]
    }
   ],
   "source": [
    "iirc_dataset_train = TorchLLLDataset(dataset=mock_dataset_iirc, tasks=tasks_iirc, setup=IIRC_SETUP, test_mode=False,\n",
    "                                     essential_transforms_fn=essential_transforms_fn, \n",
    "                                     augmentation_transforms_fn=augmentation_transforms_fn, \n",
    "                                     superclass_data_pct=0.6, subclass_data_pct=0.8)\n",
    "\n",
    "iirc_dataset_test = TorchLLLDataset(dataset=mock_dataset_iirc, tasks=tasks_iirc, setup=IIRC_SETUP, test_mode=True,\n",
    "                                     essential_transforms_fn=essential_transforms_fn, \n",
    "                                     augmentation_transforms_fn=augmentation_transforms_fn)\n",
    "\n",
    "iirc_dataset_train.choose_task(0)\n",
    "iirc_dataset_test.choose_task(0)\n",
    "\n",
    "print(f\"Length of task 0 training data: {len(iirc_dataset_train)}, number of samples for class A: {len(iirc_dataset_train.get_image_indices_by_cla('A'))}\")\n",
    "print(f\"Length of task 0 training data: {len(iirc_dataset_train)}, number of samples for class A: {len(iirc_dataset_test.get_image_indices_by_cla('A'))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second difference is the concept of incomplete information, so typically in the IIRC setup, the model is not told all the labels of a sample, but only the labels that correspond to the current task, and the model should figure the other labels on its own.\n",
    "\n",
    "This only applies to the training set though, as for the test set you need to know all the labels to evaluate the model properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0:\n",
      "training sample label: ('A', 'None')\n",
      "test sample label: ('A', 'None')\n",
      "\n",
      "task 1:\n",
      "training sample label: ('Aa', 'None')\n",
      "test sample label: ('Aa', 'A')\n",
      "\n",
      "task 2:\n",
      "training sample label: ('Ab', 'None')\n",
      "test sample label: ('A', 'Ab')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_ = [\"A\", \"Aa\", \"Ab\"]\n",
    "iirc_dataset_train.reset()\n",
    "iirc_dataset_test.reset()\n",
    "for task_id, label in enumerate(labels_):\n",
    "    iirc_dataset_train.choose_task(task_id)\n",
    "    iirc_dataset_test.choose_task(task_id)\n",
    "    train_sample, train_label1, train_label2 = \\\n",
    "        iirc_dataset_train[iirc_dataset_train.get_image_indices_by_cla(label, num_samples=1)[0]]\n",
    "    test_sample, test_label1, test_label2 = \\\n",
    "        iirc_dataset_test[iirc_dataset_test.get_image_indices_by_cla(label, num_samples=1)[0]]\n",
    "    \n",
    "    print(f\"task {task_id}:\\ntraining sample label: {(train_label1, train_label2)}\")\n",
    "    print(f\"test sample label: {test_label1, test_label2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a dataset use the *complete information* mode irrespective of whether it is a training set or a test set, the argument *complete_information_mode* can be provided when initializing the dataset, and the functions *enable_complete_information_mode()* and *enable_incomplete_information_mode()* can be used as well after initialization.\n",
    "\n",
    "Take note that the *load_tasks_up_to(task_id)* function doesn't work in the *incomplete information*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage with Incremental-CIFAR100, IIRC-CIFAR100, Incremental-Imagenet, IIRC-Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iirc.lll_datasets_loader import get_lll_datasets\n",
    "from iirc.iirc_definitions import PYTORCH\n",
    "from iirc.utils.download_cifar import download_extract_cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using these datasets with the preset tasks schedules, the original *CIFAR100* and/or *ImageNet2012* need to be downloaded first.\n",
    "\n",
    "In the case of *CIFAR100*, the dataset can be downloaded using the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting CIFAR 100\n",
      "dataset extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Work\\Master_Research\\ContinualRefinement\\ContinualRefinement\\iirc\\utils\\download_cifar.py:15: UserWarning: ./data\\cifar-100-python.tar.gz already exists, the dataset won't be download\n",
      "  warnings.warn(f\"{target_path} already exists, the dataset won't be download\")\n"
     ]
    }
   ],
   "source": [
    "download_extract_cifar100(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of *ImageNet*, it has to be downloaded manually, and be arranged in the following manner:\n",
    "* Imagenet\n",
    "    * train\n",
    "        * n01440764\n",
    "        * n01443537\n",
    "        * ...\n",
    "    * val\n",
    "        * n01440764\n",
    "        * n01443537\n",
    "        * ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the *get_lll_datasets* function should be used. The tasks schedules/configurations preset per dataset are:\n",
    "\n",
    "* *Incremental-CIFAR100*: 10 configurations, each starting with 50 classes in the first task, followed by 10 tasks each having 5 classes\n",
    "* *IIRC-CIFAR100*: 10 configurations, each starting with 10 superclasses in the first task, followed by 21 tasks each having 5 classes\n",
    "* *Incremental-Imagenet-full*: 5 configurations, each starting with 160 classes in the first task, followed by 28 tasks each having 30 classes\n",
    "* *Incremental-Imagenet-lite*: 5 configurations, each starting with 160 classes in the first task, followed by 9 tasks each having 30 classes\n",
    "* *IIRC-Imagenet-full*: 5 configurations, each starting with 63 superclasses in the first task, followed by 34 tasks each having 30 classes\n",
    "* *IIRC-Imagenet-lite*: 5 configurations, each starting with 63 superclasses in the first task, followed by 9 tasks each having 30 classes\n",
    "\n",
    "Although these configurations might seem they are limiting the choices, but the point here is to have a standard set of tasks and class orders so that the results are comparable across different works, otherwise if needed, new task configurations can be added manually as well in the *metadata* folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iirc_cifar100\n",
      "Setup used: IIRC\n",
      "Using PyTorch\n",
      "Dataset created\n"
     ]
    }
   ],
   "source": [
    "# The datasets supported are (\"incremental_cifar100\", \"iirc_cifar100\", \"incremental_imagenet_full\", \"incremental_imagenet_lite\", \n",
    "# \"iirc_imagenet_full\", \"iirc_imagenet_lite\")\n",
    "lll_datasets, tasks, class_names_to_idx = get_lll_datasets(dataset_name = \"iirc_cifar100\",\n",
    "                                                           dataset_root = \"./data\", # the parent directory of cifar-100-python or the imagenet folders\n",
    "                                                           setup = IIRC_SETUP,\n",
    "                                                           framework = PYTORCH,\n",
    "                                                           tasks_configuration_id = 0,\n",
    "                                                           essential_transforms_fn = essential_transforms_fn,\n",
    "                                                           augmentation_transforms_fn = augmentation_transforms_fn,\n",
    "                                                           joint = False \n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*joint* can also be set to *True* in case of joint training (all classes will come in one task)\n",
    "\n",
    "The result of the previous function has the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <iirc.lll_dataset.torch_lll_dataset.TorchLLLDataset at 0x286ba877070>,\n",
       " 'intask_valid': <iirc.lll_dataset.torch_lll_dataset.TorchLLLDataset at 0x286b8835fd0>,\n",
       " 'posttask_valid': <iirc.lll_dataset.torch_lll_dataset.TorchLLLDataset at 0x286b8835f70>,\n",
       " 'test': <iirc.lll_dataset.torch_lll_dataset.TorchLLLDataset at 0x286b8835dc0>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lll_datasets # four splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['flowers', 'small_mammals', 'trees', 'aquatic_mammals', 'fruit_and_vegetables', 'people', 'food_containers', 'vehicles', 'large_carnivores', 'insects'], ['television', 'spider', 'shrew', 'mountain', 'hamster'], ['road', 'poppy', 'household_furniture', 'woman', 'bee']]\n"
     ]
    }
   ],
   "source": [
    "print(tasks[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flowers': 0, 'small_mammals': 1, 'trees': 2, 'aquatic_mammals': 3, 'fruit_and_vegetables': 4, 'people': 5, 'food_containers': 6, 'vehicles': 7, 'large_carnivores': 8, 'insects': 9, 'television': 10, 'spider': 11, 'shrew': 12, 'mountain': 13, 'hamster': 14, 'road': 15, 'poppy': 16, 'household_furniture': 17, 'woman': 18, 'bee': 19, 'tulip': 20, 'clock': 21, 'orange': 22, 'beaver': 23, 'rocket': 24, 'bicycle': 25, 'can': 26, 'squirrel': 27, 'wardrobe': 28, 'bus': 29, 'whale': 30, 'sweet_pepper': 31, 'telephone': 32, 'leopard': 33, 'bowl': 34, 'skyscraper': 35, 'baby': 36, 'cockroach': 37, 'boy': 38, 'lobster': 39, 'motorcycle': 40, 'forest': 41, 'tank': 42, 'orchid': 43, 'chair': 44, 'crab': 45, 'girl': 46, 'keyboard': 47, 'otter': 48, 'bed': 49, 'butterfly': 50, 'lawn_mower': 51, 'snail': 52, 'caterpillar': 53, 'wolf': 54, 'pear': 55, 'tiger': 56, 'pickup_truck': 57, 'cup': 58, 'reptiles': 59, 'train': 60, 'sunflower': 61, 'beetle': 62, 'apple': 63, 'palm_tree': 64, 'plain': 65, 'large_omnivores_and_herbivores': 66, 'rose': 67, 'tractor': 68, 'crocodile': 69, 'mushroom': 70, 'couch': 71, 'lamp': 72, 'mouse': 73, 'bridge': 74, 'turtle': 75, 'willow_tree': 76, 'man': 77, 'lizard': 78, 'maple_tree': 79, 'lion': 80, 'elephant': 81, 'seal': 82, 'sea': 83, 'dinosaur': 84, 'worm': 85, 'bear': 86, 'castle': 87, 'plate': 88, 'dolphin': 89, 'medium_sized_mammals': 90, 'streetcar': 91, 'bottle': 92, 'kangaroo': 93, 'snake': 94, 'house': 95, 'chimpanzee': 96, 'raccoon': 97, 'porcupine': 98, 'oak_tree': 99, 'pine_tree': 100, 'possum': 101, 'skunk': 102, 'fish': 103, 'fox': 104, 'cattle': 105, 'ray': 106, 'aquarium_fish': 107, 'cloud': 108, 'flatfish': 109, 'rabbit': 110, 'trout': 111, 'camel': 112, 'table': 113, 'shark': 114}\n"
     ]
    }
   ],
   "source": [
    "print(class_names_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*lll_datasets* has four splits, where *train* is for training, *intask_valid* is for validation during task training (in case of IIRC setup, this split is using *incomplete information* like the *train* split), *posttask_valid* is for validation after each task training (in case of IIRC setup, this split is using *complete information* like the *test* split), and finally the *test* split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
